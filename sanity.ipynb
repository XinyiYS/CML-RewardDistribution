{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "import gpytorch\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, tanh\n",
    "from torch.nn import Linear, Conv2d, ConvTranspose2d, BatchNorm2d, ReLU, LeakyReLU\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.mmd import mmd_neg_biased, mmd_neg_unbiased\n",
    "from data.pipeline import get_data_features\n",
    "from core.kernel import get_kernel\n",
    "from core.reward_calculation import get_v\n",
    "from data.pipeline import get_data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cifar'\n",
    "num_classes = 10\n",
    "d = 8\n",
    "num_parties = 5\n",
    "party_data_size = 5000\n",
    "candidate_data_size = 100000\n",
    "split = 'equaldisjoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_datasets, party_labels, reference_dataset, candidate_datasets, candidate_labels = get_data_features(dataset,\n",
    "                                                                                            num_classes,\n",
    "                                                                                            d,\n",
    "                                                                                            num_parties,\n",
    "                                                                                            party_data_size,\n",
    "                                                                                            candidate_data_size,\n",
    "                                                                                            split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_datasets, party_labels, reference_dataset, candidate_datasets, candidate_labels = get_data_features('gmm',\n",
    "                                                                                            5,\n",
    "                                                                                            2,\n",
    "                                                                                            5,\n",
    "                                                                                            1000,\n",
    "                                                                                            5000,\n",
    "                                                                                            'equaldisjoint',\n",
    "                                                                                            gamma=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_idx = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "party_ds_size = train_test_split_idx\n",
    "num_parties = len(party_datasets)\n",
    "party_datasets_tens = torch.tensor(party_datasets[:, :train_test_split_idx], device=device, dtype=torch.float32)\n",
    "reference_dataset_tens = torch.tensor(reference_dataset, device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_datasets_test = torch.tensor(party_datasets[:, train_test_split_idx:], device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parties = 5\n",
    "num_epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ard_num_dims = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEKernel():\n",
    "    \"\"\"\n",
    "    Custom squared exponential kernel parameterized by inverse lengthscale\n",
    "    \"\"\"\n",
    "    def __init__(self, ard_num_dims, inv_lengthscale_squared, device):\n",
    "        self.inv_ls_squared = torch.tensor([inv_lengthscale_squared for i in range(ard_num_dims)], device=device, requires_grad=True, dtype=torch.float32)\n",
    "        self.ard_num_dims = ard_num_dims\n",
    "        self.device = device\n",
    "        \n",
    "    def __call__(self, X, Y=None):\n",
    "        \"\"\"\n",
    "        :param X: torch tensor of size (m, d)\n",
    "        :param Y: torch tensor of size (n, d)\n",
    "        :return: lazy tensor of size (m, n)\n",
    "        \"\"\"\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        \n",
    "        diff_squared = torch.square(torch.unsqueeze(X, 1) - Y)  # tensor of shape (m, n, d)\n",
    "        exponent = torch.matmul(diff_squared, self.inv_ls_squared)  # tensor of shape (m, n)\n",
    "        return torch.exp(-0.5 * exponent)\n",
    "    \n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.inv_ls_squared]\n",
    "    \n",
    "    \n",
    "    def set_inv_ls_squared_scalar(self, inv_ls):\n",
    "        self.inv_ls_squared = torch.tensor([inv_ls for i in range(self.ard_num_dims)], device=self.device, requires_grad=True, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    def set_inv_ls_squared(self, inv_ls_squared):\n",
    "        self.inv_ls_squared = torch.tensor(inv_ls_squared, device=self.device, requires_grad=True, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kernel = SEKernel(2, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd_neg_unbiased_noeval(X, Y, k):\n",
    "    \"\"\"\n",
    "    Used as loss function.\n",
    "    :param X: Torch tensor\n",
    "    :param Y: Torch tensor\n",
    "    :param k: GPyTorch kernel\n",
    "    :return: scalar\n",
    "    \"\"\"\n",
    "    m = X.size(0)\n",
    "    n = Y.size(0)\n",
    "\n",
    "    S_X = (1 / (m * (m-1))) * (torch.sum(k(X)) - torch.sum(torch.diag(k(X))))\n",
    "    S_XY = (2 / (m * n)) * torch.sum(k(X, Y))\n",
    "    S_Y = (1 / (n * (n-1))) * (torch.sum(k(Y)) - torch.sum(torch.diag(k(Y))))\n",
    "\n",
    "    return S_XY - S_X - S_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonneg_lb(n, S, k, eta=None, tol=1e-04):\n",
    "    \"\"\"\n",
    "    :param n: size of reference dataset Y\n",
    "    :param S: minimum party dataset size\n",
    "    :param k: value of diagonal terms (usually 1)\n",
    "    :param eta: upper bound (if none, set to k)\n",
    "    :return: scalar\n",
    "    \"\"\"\n",
    "    if eta is None:\n",
    "        eta = k\n",
    "    \n",
    "    return (n-2*S)/(2*S*(n-S)) * (k + (S-1) * eta) - tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(reference_dataset)\n",
    "S = np.min([len(ds) for ds in party_datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = nonneg_lb(n, S, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pareto_efficient(costs, return_mask = True):\n",
    "    \"\"\"\n",
    "    Find the pareto-efficient points\n",
    "    :param costs: An (n_points, n_costs) array\n",
    "    :param return_mask: True to return a mask\n",
    "    :return: An array of indices of pareto-efficient points.\n",
    "        If return_mask is True, this will be an (n_points, ) boolean array\n",
    "        Otherwise it will be a (n_efficient_points, ) integer array of indices.\n",
    "    \"\"\"\n",
    "    is_efficient = np.arange(costs.shape[0])\n",
    "    n_points = costs.shape[0]\n",
    "    next_point_index = 0  # Next index in the is_efficient array to search for\n",
    "    while next_point_index<len(costs):\n",
    "        nondominated_point_mask = np.any(costs<costs[next_point_index], axis=1)\n",
    "        nondominated_point_mask[next_point_index] = True\n",
    "        is_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points\n",
    "        costs = costs[nondominated_point_mask]\n",
    "        next_point_index = np.sum(nondominated_point_mask[:next_point_index])+1\n",
    "    if return_mask:\n",
    "        is_efficient_mask = np.zeros(n_points, dtype = bool)\n",
    "        is_efficient_mask[is_efficient] = True\n",
    "        return is_efficient_mask\n",
    "    else:\n",
    "        return is_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select num_val_points random points to check k(x_i, x_j) > lb\n",
    "num_val_points = 2000\n",
    "val_points = torch.tensor(reference_dataset[np.random.permutation(np.arange(num_val_points))], device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_all_above_lb(k, val_points, lb):\n",
    "    num_above = (k(val_points).cpu().detach().numpy() > lb).sum()\n",
    "    return num_above == len(val_points) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_points_np = val_points.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_diffs = np.square(np.expand_dims(val_points_np, 1) - val_points_np)  # (m, m, d)\n",
    "squared_diffs = np.reshape(squared_diffs, [-1, d])\n",
    "squared_diff_idxs = np.where((np.triu(np.ones((num_val_points, num_val_points))) - np.diag(np.ones(num_val_points))).flatten())[0]\n",
    "squared_diffs_reduced = squared_diffs[squared_diff_idxs]\n",
    "reduced_D = squared_diffs_reduced[is_pareto_efficient(-squared_diffs_reduced)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (-2 * np.log(lb)) * np.ones(len(reduced_D), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = SEKernel(2, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Do a binary search for a good value of inv_ls_squared, low but above upper bound\n",
    "# num_iters = 20\n",
    "# low = 1\n",
    "# high = 1000\n",
    "# current = low\n",
    "\n",
    "# # Check bounds\n",
    "# k.set_inv_ls_squared_scalar(low)\n",
    "# if not is_all_above_lb(k, val_points, lb):\n",
    "#     raise Exception(\"Low value of inv_ls_squared is already invalid\")\n",
    "    \n",
    "# k.set_inv_ls_squared_scalar(high)\n",
    "# if is_all_above_lb(k, val_points, lb):\n",
    "#     raise Exception(\"High value of inv_ls_squared is still valid, can be pushed higher\")\n",
    "\n",
    "# for i in range(num_iters):\n",
    "#     mid = (high + low) / 2\n",
    "#     print(mid)\n",
    "#     k.set_inv_ls_squared_scalar(mid)\n",
    "#     if is_all_above_lb(k, val_points, lb):\n",
    "#         low = mid\n",
    "#     else:\n",
    "#         high = mid\n",
    "\n",
    "# k.set_inv_ls_squared_scalar(low)\n",
    "# print(\"Optimal inverse lengthscale squared: {}\".format(low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(k.parameters(), lr=0.1)\n",
    "t = 0\n",
    "patience = 20\n",
    "averages = []\n",
    "best_idx = 0\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    \n",
    "    print(\"========= Test -MMD unbiased ===========\")\n",
    "    stats = []\n",
    "    for i in range(num_parties):\n",
    "        stat = mmd_neg_unbiased_noeval(party_datasets_test[i], reference_dataset_tens, k).cpu().detach().numpy()\n",
    "        print(\"Party {}: {}\".format(i+1, stat))\n",
    "        stats.append(stat)\n",
    "    avg = np.mean(stats)\n",
    "    print(\"Average: {}\".format(avg))\n",
    "    \n",
    "    print(\"========= Kernel parameters ===========\")\n",
    "    print(\"inv lengthscale squared:\")\n",
    "    print(k.inv_ls_squared)\n",
    "    print(\"lengthscale:\")\n",
    "    print(np.sqrt(1 / k.inv_ls_squared.cpu().detach().numpy()))\n",
    "    print(\"k still valid (all above upper bound): {}\".format(is_all_above_lb(k, val_points, lb)))\n",
    "    \n",
    "    for i in range(party_ds_size // batch_size):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "\n",
    "        idx = (i + 1)\n",
    "        next_m = np.min([idx * batch_size, party_ds_size])\n",
    "        m = i * batch_size\n",
    "\n",
    "        ref_idx = np.random.randint(0, len(reference_dataset) - batch_size)\n",
    "        next_ref_idx = ref_idx + batch_size\n",
    "        \n",
    "        #print(m, next_m)\n",
    "\n",
    "        for party in range(num_parties):\n",
    "            loss += mmd_neg_unbiased_noeval(party_datasets_tens[party][m:next_m],\n",
    "                                     reference_dataset_tens[ref_idx:next_ref_idx],\n",
    "                                     k)\n",
    "\n",
    "        # Calc loss and backprop gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # change gradients to argmin x \\in C <grad, x>\n",
    "        grad = k.inv_ls_squared.grad.cpu().numpy()\n",
    "        print(\"Actual grad: {}\".format(grad))\n",
    "        res = linprog(grad, A_ub=reduced_D, b_ub=b, method='interior-point')\n",
    "        y_t = res['x']\n",
    "        print(\"y_t: {}\".format(y_t))\n",
    "        print(\"inv_ls_squared: {}\".format(k.inv_ls_squared))\n",
    "        \n",
    "        # original conditional gradient update method\n",
    "        step_size = 2/(t + 2)\n",
    "        print(\"Step size: {}\".format(step_size))\n",
    "        k.set_inv_ls_squared((1 - step_size) * k.inv_ls_squared.cpu().detach().numpy() + step_size * y_t)\n",
    "        t += 1\n",
    "        \n",
    "#         # gradient descent with constant step size\n",
    "#         step_size = 0.1\n",
    "#         k.set_inv_ls_squared((1 - step_size) * k.inv_ls_squared.cpu().detach().numpy() + step_size * y_t)\n",
    "        \n",
    "        #k.inv_ls_squared.grad = torch.tensor(new_grad, device=device, dtype=torch.float32)\n",
    "        #optimizer.step()\n",
    "        #print(\"k still valid (all above upper bound): {}\".format(is_all_above_lb(k, val_points, lb)))\n",
    "        #print(\"inv lengthscale squared:\")\n",
    "        #print(k.inv_ls_squared)\n",
    "        #print(\"lengthscale:\")\n",
    "        #print(np.sqrt(1 / k.inv_ls_squared.cpu().detach().numpy()))\n",
    "\n",
    "    \n",
    "    # Code for early termination if no improvement after patience number of epochs\n",
    "    averages.append(avg)\n",
    "    if avg <= averages[best_idx]:\n",
    "        best_idx = epoch  # Low is better for this\n",
    "    elif avg >= averages[best_idx] and epoch - best_idx >= patience:\n",
    "        print(\"No improvement for {} epochs, terminating early\".format(patience))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2 = get_kernel('se', d, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2.base_kernel.lengthscale = np.sqrt(1 / k.inv_ls_squared.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(mmd_neg_biased(party_datasets[i], reference_dataset, k2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = get_kernel('se', 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k.base_kernel.lengthscale = 0.5\n",
    "X = torch.tensor([1, 2, 3, 4, -1, 10, 3, 2, 1])\n",
    "mat = k(X,X).evaluate().detach().numpy()\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range(len(X)):\n",
    "        lb = 0.4375\n",
    "        if mat[i, j] < lb:\n",
    "            mat[i, j] = lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(X)):\n",
    "    S = X[:i]\n",
    "    lower_bound = gamma(X, S)\n",
    "    print(\"lb: {}\".format(lower_bound))\n",
    "    \n",
    "    pos_term = 2/(len(S) * len(X)) * np.sum(mat[len(S):, :len(S)])\n",
    "    neg_term = (2/(len(S) * len(X)) - 1/(len(S) ** 2)) * np.sum(mat[:len(S), :len(S)])\n",
    "    \n",
    "    print(\"v: {}\".format(pos_term + neg_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range(len(X)):\n",
    "        if i != j:\n",
    "            mat[i, j] = 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma(X, S):\n",
    "    n = len(X)\n",
    "    s = len(S)\n",
    "    return ((n/(2*s)-1)/(n-s)) * (1+(s-1)*eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = ((n/(2*s)-1)/(n-s)) * (1+(s-1)*eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lower_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, len(X)):\n",
    "    S = X[:i]\n",
    "    pos_term = 2/(len(S) * len(X)) * np.sum(mat[len(S):, :len(S)])\n",
    "    neg_term = (2/(len(S) * len(X)) - 1/(len(S) ** 2)) * np.sum(mat[:len(S), :len(S)])\n",
    "    \n",
    "    SXY = 2 / (len(S) * len(X)) * np.sum(mat[:, :len(S)])\n",
    "    SX =  1/(len(S) ** 2) * np.sum(mat[:len(S), :len(S)])\n",
    "    print(\"SXY: {}\".format(SXY))\n",
    "    print(\"SX: {}\".format(SX))\n",
    "    \n",
    "    print(pos_term + neg_term)\n",
    "    print(\"SXY - SX: {}\".format(SXY - SX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mmd_neg_biased(S, X, k)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_term = 2/(len(S) * 10) * np.sum(mat[len(S):, :len(S)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_term = (2/(len(S) * 10) - 1/(len(S) ** 2)) * np.sum(mat[:len(S), :len(S)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_term + neg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.load(\"data/cifar/cifar_train_images.npy\")\n",
    "train_labels = np.load(\"data/cifar/cifar_train_labels.npy\")\n",
    "candidate_images = np.load(\"data/cifar/cifar_samples.npy\")\n",
    "candidate_labels = np.load(\"data/cifar/cifar_samples_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 6, 7, 8]  # Classes we want, airplane, automobile, frog, horse, ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {0: 0,\n",
    "             1: 1,\n",
    "             6: 2,\n",
    "             7: 3,\n",
    "             8: 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = []\n",
    "for i in range(len(train_images)):\n",
    "    if train_labels[i] in classes:\n",
    "        train_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_train_images = train_images[train_idx]\n",
    "cifar5_train_labels = train_labels[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_train_new_labels = [class_dict[label] for label in cifar5_train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(cifar5_train_images[200:264], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cifar5_train_new_labels[200:264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_idx = []\n",
    "for i in range(len(candidate_images)):\n",
    "    if candidate_labels[i] in classes:\n",
    "        candidate_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_candidate_images = candidate_images[candidate_idx]\n",
    "cifar5_candidate_labels = candidate_labels[candidate_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_candidate_new_labels = [class_dict[label] for label in cifar5_candidate_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(cifar5_candidate_images[200:264], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cifar5_candidate_new_labels[200:264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/cifar5/cifar5_train_images.npy\", cifar5_train_images)\n",
    "np.save(\"data/cifar5/cifar5_train_labels.npy\", cifar5_train_new_labels)\n",
    "np.save(\"data/cifar5/cifar5_samples.npy\", cifar5_candidate_images)\n",
    "np.save(\"data/cifar5/cifar5_samples_labels.npy\", cifar5_candidate_new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_datasets, party_labels, candidate_dataset, candidate_labels = get_data_raw(dataset='cifar5',\n",
    "                                                                                     num_classes=5,\n",
    "                                                                                     party_data_size=5000,\n",
    "                                                                                     candidate_data_size=20000,\n",
    "                                                                                     split='unequal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_idx = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(party_datasets[party_idx, 200:264], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels[party_idx, 200:264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(candidate_dataset[200:264], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels[200:264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD GPU batch calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_datasets, party_labels, reference_dataset, candidate_datasets, candidate_labels = get_data_features('gmm',\n",
    "                                                                                            5,\n",
    "                                                                                            2,\n",
    "                                                                                            5,\n",
    "                                                                                            1000,\n",
    "                                                                                            5000,\n",
    "                                                                                            'equaldisjoint',\n",
    "                                                                                            gamma=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = get_kernel('se', 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### BUGGED CODE, WILL NOT GIVE CORRECT RESULT WHEN BATCH_SIZE > PARTY DATA SIZE\n",
    "\n",
    "def mmd_neg_biased_batched(X, Y, k, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    Calculates biased MMD^2 without the S_YY term, where S_X, S_XY and S_YY are the pairwise-XX, pairwise-XY, pairwise-YY\n",
    "    summation terms respectively. Does so using the GPU in a batch-wise manner.\n",
    "    :param X: array of shape (m, d)\n",
    "    :param Y: array of shape (n, d)\n",
    "    :param k: GPyTorch kernel\n",
    "    :param device:\n",
    "    :param batch_size:\n",
    "    :return: MMD^2, S_X, S_XY\n",
    "    \"\"\"\n",
    "    max_m = X.shape[0]\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    X_tens = torch.tensor(X, device=device)\n",
    "    Y_tens = torch.tensor(Y, device=device)\n",
    "    k.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # first batch\n",
    "        S_XY = (2 / (batch_size * n)) * torch.sum(k(X_tens[:batch_size], Y_tens).evaluate())\n",
    "        S_X = (1 / (batch_size ** 2)) * torch.sum(k(X_tens[:batch_size]).evaluate())\n",
    "\n",
    "        for i in range(max_m // batch_size):\n",
    "            idx = i + 2\n",
    "            next_m = np.min([idx * batch_size, max_m])\n",
    "            m = (idx - 1) * batch_size\n",
    "            S_XY = (m * S_XY + (2 / n) * torch.sum(k(X_tens[m:next_m], Y_tens).evaluate())) / next_m\n",
    "            S_X = ((m ** 2) * S_X + 2 * torch.sum(k(X_tens[m:next_m], X_tens[:m]).evaluate()) +\n",
    "                   torch.sum(k(X_tens[m:next_m]).evaluate())) / (next_m ** 2)\n",
    "    \n",
    "    return (S_XY - S_X).item(), S_X.item(), S_XY.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = party_datasets[2]\n",
    "Y = reference_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mmd_neg_biased(X, Y, kernel.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mmd_neg_biased_batched(X, Y, kernel, 'cuda:0', batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_tens = torch.tensor(Y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tens.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tens = torch.tensor(X).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel = kernel.to(device)\n",
    "with torch.no_grad():\n",
    "    print(torch.sum(kernel(Y_tens, Y_tens).evaluate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.to('cpu')\n",
    "with torch.no_grad():\n",
    "    print(torch.sum(kernel(torch.tensor(X), torch.tensor(Y)).evaluate() * 1/100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel(X_tens[:64], Y_tens).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Y_tens.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_m = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel.to(device)\n",
    "X_tens.to(device)\n",
    "Y_tens.to(device)\n",
    "with torch.no_grad():\n",
    "    # first batch\n",
    "    S_XY = (2/(batch_size * n)) * torch.sum(kernel(X_tens[:batch_size], Y_tens).evaluate())\n",
    "    S_X = (1/(batch_size ** 2)) * torch.sum(kernel(X_tens[:batch_size]).evaluate())\n",
    "    \n",
    "    for i in range(max_m // batch_size):\n",
    "        idx = i + 2\n",
    "        m = np.min([idx * batch_size, max_m])\n",
    "        prev_m = (idx - 1) * batch_size\n",
    "        c = m - prev_m\n",
    "        print(prev_m, m)\n",
    "        S_XY = (prev_m * S_XY + (2/n) * torch.sum(kernel(X_tens[prev_m:m], Y_tens).evaluate())) / (prev_m + c)\n",
    "        S_X = ((prev_m ** 2) * S_X + 2 * torch.sum(kernel(X_tens[prev_m:m], X_tens[:prev_m]).evaluate()) + \n",
    "                torch.sum(kernel(X_tens[prev_m:m]).evaluate())) / ((prev_m + c) ** 2) \n",
    "        print((S_XY.item() - S_X.item(), S_X.item(), S_XY.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mmd_neg_biased(X, Y, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.to('cpu')\n",
    "for i in range(max_m // batch_size):\n",
    "    idx = i + 2\n",
    "    m = np.min([idx * batch_size, max_n])\n",
    "    print(mmd_neg_biased(X[:m], Y, kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.to(device)\n",
    "with torch.no_grad():\n",
    "    # first batch\n",
    "    S_XY = (2/(batch_size * n)) * torch.sum(kernel(X_tens[:batch_size], Y_tens).evaluate())\n",
    "    S_X = (1/(batch_size ** 2)) * torch.sum(kernel(X_tens[:batch_size]).evaluate())\n",
    "    print((S_XY.item() - S_X.item(), S_X.item(), S_XY.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel.to('cpu')\n",
    "print(mmd_neg_biased(X[:32], Y, kernel))\n",
    "_, S_X, S_XY = mmd_neg_biased(X[:32], Y, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(32 * S_XY + (2/n) * torch.sum(kernel(torch.tensor(X[32:64]), torch.tensor(Y)).evaluate())) / (32 + 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_XY - S_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1000 // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.min([(i+2) * batch_size, 1000]) for i in range(1000 // batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32\n",
    "num_channels = 3\n",
    "hidden_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(4, num_channels, input_dim, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Conv2d(in_channels=num_channels, out_channels=64, kernel_size=5, stride=2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.std(conv1(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_bn = BatchNorm2d(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(conv1_bn(conv1(inp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv2 = Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2(conv1(inp)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride=2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3(conv2(conv1(inp))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv4 = Conv2d(in_channels=256, out_channels=512, kernel_size=5, stride=2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv4(conv3(conv2(conv1(inp)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "that_size = conv4(conv3(conv2(conv1(inp)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = Linear(2048, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1(conv4(conv3(conv2(conv1(inp)))).view((4, -1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomView(nn.Module):  # Flattening layer for nn.Sequential\n",
    "    def __init__(self, shape):\n",
    "        super(CustomView, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            CustomView((-1, 2048)),\n",
    "            nn.Linear(2048, hidden_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            CustomView((-1, 512, 2, 2)),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=0 if input_dim==28 else 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, num_channels, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder(encoder(inp)).shape#.permute((0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = (1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(means, axis=[1, 2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.expand_dims(means, axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder(encoder(inp)).detach().numpy() * means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--batch_size', default=64, type=int)\n",
    "parser.add_argument('--hidden_dim', default=16, type=int)\n",
    "parser.add_argument('--dataset', default='cifar', type=str)\n",
    "parser.add_argument('--num_classes', default=10, type=int)\n",
    "parser.add_argument('--party_data_size', default=4000, type=int)\n",
    "parser.add_argument('--candidate_data_size', default=10000, type=int)\n",
    "parser.add_argument('--split', default='equaldisjoint', type=str)\n",
    "parser.add_argument('-f', type=str)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_datasets, party_labels, candidate_dataset, candidate_labels = get_data_raw(dataset=args.dataset,\n",
    "                                                                                         num_classes=args.num_classes,\n",
    "                                                                                         party_data_size=args.party_data_size,\n",
    "                                                                                         candidate_data_size=args.candidate_data_size,\n",
    "                                                                                         split=args.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(party_datasets[party][:64], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "party_labels[party][:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(candidate_dataset[:64], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidate_labels[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(party_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(party_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(candidate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.min(candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = party_datasets.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(candidate_dataset.reshape([-1, num_channels]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.concatenate([np.concatenate(party_datasets), candidate_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means = np.mean(combined.reshape(-1, num_channels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = np.std(combined.reshape(-1, num_channels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for i in range(len(party_datasets)):\n",
    "    transformed = (party_datasets[i] - means) / stds\n",
    "    dataset = TensorDataset(torch.tensor(transformed), torch.tensor(party_labels[i]))\n",
    "    datasets.append(dataset)\n",
    "transformed = (candidate_dataset - means) / stds\n",
    "dataset = TensorDataset(torch.tensor(transformed), torch.tensor(candidate_labels))\n",
    "datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dataset = ConcatDataset(*datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "            concat_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iterator = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_labels = next(my_iterator)[party]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(merge(batch_images, [8, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
