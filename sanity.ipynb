{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import gpytorch\n",
    "\n",
    "from data.pipeline import get_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, tanh\n",
    "from torch.nn import Linear, Conv2d, ConvTranspose2d, BatchNorm2d, ReLU, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.mmd import mmd_neg_biased\n",
    "from data.pipeline import get_data_features\n",
    "from core.kernel import get_kernel\n",
    "from core.reward_calculation import get_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.load(\"data/cifar/cifar_train_images.npy\")\n",
    "train_labels = np.load(\"data/cifar/cifar_train_labels.npy\")\n",
    "candidate_images = np.load(\"data/cifar/cifar_samples.npy\")\n",
    "candidate_labels = np.load(\"data/cifar/cifar_samples_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 6, 7, 8]  # Classes we want, airplane, automobile, frog, horse, ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {0: 0,\n",
    "             1: 1,\n",
    "             6: 2,\n",
    "             7: 3,\n",
    "             8: 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = []\n",
    "for i in range(len(train_images)):\n",
    "    if train_labels[i] in classes:\n",
    "        train_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_train_images = train_images[train_idx]\n",
    "cifar5_train_labels = train_labels[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_train_new_labels = [class_dict[label] for label in cifar5_train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(cifar5_train_images[200:264], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cifar5_train_new_labels[200:264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_idx = []\n",
    "for i in range(len(candidate_images)):\n",
    "    if candidate_labels[i] in classes:\n",
    "        candidate_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_candidate_images = candidate_images[candidate_idx]\n",
    "cifar5_candidate_labels = candidate_labels[candidate_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar5_candidate_new_labels = [class_dict[label] for label in cifar5_candidate_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(cifar5_candidate_images[200:264], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cifar5_candidate_new_labels[200:264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/cifar5/cifar5_train_images.npy\", cifar5_train_images)\n",
    "np.save(\"data/cifar5/cifar5_train_labels.npy\", cifar5_train_new_labels)\n",
    "np.save(\"data/cifar5/cifar5_samples.npy\", cifar5_candidate_images)\n",
    "np.save(\"data/cifar5/cifar5_samples_labels.npy\", cifar5_candidate_new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_datasets, party_labels, candidate_dataset, candidate_labels = get_data_raw(dataset='cifar5',\n",
    "                                                                                     num_classes=5,\n",
    "                                                                                     party_data_size=5000,\n",
    "                                                                                     candidate_data_size=20000,\n",
    "                                                                                     split='unequal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_idx = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(party_datasets[party_idx, 200:264], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels[party_idx, 200:264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(candidate_dataset[200:264], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels[200:264]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD GPU batch calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_datasets, party_labels, reference_dataset, candidate_datasets, candidate_labels = get_data_features('gmm',\n",
    "                                                                                            5,\n",
    "                                                                                            2,\n",
    "                                                                                            5,\n",
    "                                                                                            1000,\n",
    "                                                                                            5000,\n",
    "                                                                                            'equaldisjoint',\n",
    "                                                                                            gamma=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = get_kernel('se', 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### BUGGED CODE, WILL NOT GIVE CORRECT RESULT WHEN BATCH_SIZE > PARTY DATA SIZE\n",
    "\n",
    "def mmd_neg_biased_batched(X, Y, k, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    Calculates biased MMD^2 without the S_YY term, where S_X, S_XY and S_YY are the pairwise-XX, pairwise-XY, pairwise-YY\n",
    "    summation terms respectively. Does so using the GPU in a batch-wise manner.\n",
    "    :param X: array of shape (m, d)\n",
    "    :param Y: array of shape (n, d)\n",
    "    :param k: GPyTorch kernel\n",
    "    :param device:\n",
    "    :param batch_size:\n",
    "    :return: MMD^2, S_X, S_XY\n",
    "    \"\"\"\n",
    "    max_m = X.shape[0]\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    X_tens = torch.tensor(X, device=device)\n",
    "    Y_tens = torch.tensor(Y, device=device)\n",
    "    k.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # first batch\n",
    "        S_XY = (2 / (batch_size * n)) * torch.sum(k(X_tens[:batch_size], Y_tens).evaluate())\n",
    "        S_X = (1 / (batch_size ** 2)) * torch.sum(k(X_tens[:batch_size]).evaluate())\n",
    "\n",
    "        for i in range(max_m // batch_size):\n",
    "            idx = i + 2\n",
    "            next_m = np.min([idx * batch_size, max_m])\n",
    "            m = (idx - 1) * batch_size\n",
    "            S_XY = (m * S_XY + (2 / n) * torch.sum(k(X_tens[m:next_m], Y_tens).evaluate())) / next_m\n",
    "            S_X = ((m ** 2) * S_X + 2 * torch.sum(k(X_tens[m:next_m], X_tens[:m]).evaluate()) +\n",
    "                   torch.sum(k(X_tens[m:next_m]).evaluate())) / (next_m ** 2)\n",
    "    \n",
    "    return (S_XY - S_X).item(), S_X.item(), S_XY.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = party_datasets[2]\n",
    "Y = reference_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9387285113334656, 0.9873847365379333, 1.926113247871399)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmd_neg_biased(X, Y, kernel.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7162104561466199, 0.24684620067407545, 0.9630566568206954)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmd_neg_biased_batched(X, Y, kernel, 'cuda:0', batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_tens = torch.tensor(Y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tens.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tens = torch.tensor(X).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel = kernel.to(device)\n",
    "with torch.no_grad():\n",
    "    print(torch.sum(kernel(Y_tens, Y_tens).evaluate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.to('cpu')\n",
    "with torch.no_grad():\n",
    "    print(torch.sum(kernel(torch.tensor(X), torch.tensor(Y)).evaluate() * 1/100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel(X_tens[:64], Y_tens).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Y_tens.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_m = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel.to(device)\n",
    "X_tens.to(device)\n",
    "Y_tens.to(device)\n",
    "with torch.no_grad():\n",
    "    # first batch\n",
    "    S_XY = (2/(batch_size * n)) * torch.sum(kernel(X_tens[:batch_size], Y_tens).evaluate())\n",
    "    S_X = (1/(batch_size ** 2)) * torch.sum(kernel(X_tens[:batch_size]).evaluate())\n",
    "    \n",
    "    for i in range(max_m // batch_size):\n",
    "        idx = i + 2\n",
    "        m = np.min([idx * batch_size, max_m])\n",
    "        prev_m = (idx - 1) * batch_size\n",
    "        c = m - prev_m\n",
    "        print(prev_m, m)\n",
    "        S_XY = (prev_m * S_XY + (2/n) * torch.sum(kernel(X_tens[prev_m:m], Y_tens).evaluate())) / (prev_m + c)\n",
    "        S_X = ((prev_m ** 2) * S_X + 2 * torch.sum(kernel(X_tens[prev_m:m], X_tens[:prev_m]).evaluate()) + \n",
    "                torch.sum(kernel(X_tens[prev_m:m]).evaluate())) / ((prev_m + c) ** 2) \n",
    "        print((S_XY.item() - S_X.item(), S_X.item(), S_XY.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mmd_neg_biased(X, Y, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.to('cpu')\n",
    "for i in range(max_m // batch_size):\n",
    "    idx = i + 2\n",
    "    m = np.min([idx * batch_size, max_n])\n",
    "    print(mmd_neg_biased(X[:m], Y, kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.to(device)\n",
    "with torch.no_grad():\n",
    "    # first batch\n",
    "    S_XY = (2/(batch_size * n)) * torch.sum(kernel(X_tens[:batch_size], Y_tens).evaluate())\n",
    "    S_X = (1/(batch_size ** 2)) * torch.sum(kernel(X_tens[:batch_size]).evaluate())\n",
    "    print((S_XY.item() - S_X.item(), S_X.item(), S_XY.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel.to('cpu')\n",
    "print(mmd_neg_biased(X[:32], Y, kernel))\n",
    "_, S_X, S_XY = mmd_neg_biased(X[:32], Y, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(32 * S_XY + (2/n) * torch.sum(kernel(torch.tensor(X[32:64]), torch.tensor(Y)).evaluate())) / (32 + 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_XY - S_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1000 // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.min([(i+2) * batch_size, 1000]) for i in range(1000 // batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32\n",
    "num_channels = 3\n",
    "hidden_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(4, num_channels, input_dim, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Conv2d(in_channels=num_channels, out_channels=64, kernel_size=5, stride=2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.std(conv1(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_bn = BatchNorm2d(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(conv1_bn(conv1(inp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv2 = Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2(conv1(inp)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride=2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3(conv2(conv1(inp))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv4 = Conv2d(in_channels=256, out_channels=512, kernel_size=5, stride=2, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv4(conv3(conv2(conv1(inp)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "that_size = conv4(conv3(conv2(conv1(inp)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = Linear(2048, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1(conv4(conv3(conv2(conv1(inp)))).view((4, -1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomView(nn.Module):  # Flattening layer for nn.Sequential\n",
    "    def __init__(self, shape):\n",
    "        super(CustomView, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            CustomView((-1, 2048)),\n",
    "            nn.Linear(2048, hidden_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            CustomView((-1, 512, 2, 2)),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=0 if input_dim==28 else 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, num_channels, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder(encoder(inp)).shape#.permute((0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = (1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(means, axis=[1, 2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.expand_dims(means, axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder(encoder(inp)).detach().numpy() * means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--batch_size', default=64, type=int)\n",
    "parser.add_argument('--hidden_dim', default=16, type=int)\n",
    "parser.add_argument('--dataset', default='cifar', type=str)\n",
    "parser.add_argument('--num_classes', default=10, type=int)\n",
    "parser.add_argument('--party_data_size', default=4000, type=int)\n",
    "parser.add_argument('--candidate_data_size', default=10000, type=int)\n",
    "parser.add_argument('--split', default='equaldisjoint', type=str)\n",
    "parser.add_argument('-f', type=str)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_datasets, party_labels, candidate_dataset, candidate_labels = get_data_raw(dataset=args.dataset,\n",
    "                                                                                         num_classes=args.num_classes,\n",
    "                                                                                         party_data_size=args.party_data_size,\n",
    "                                                                                         candidate_data_size=args.candidate_data_size,\n",
    "                                                                                         split=args.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(party_datasets[party][:64], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "party_labels[party][:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merge(candidate_dataset[:64], [8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidate_labels[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(party_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(party_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(candidate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.min(candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = party_datasets.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(candidate_dataset.reshape([-1, num_channels]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.concatenate([np.concatenate(party_datasets), candidate_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means = np.mean(combined.reshape(-1, num_channels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = np.std(combined.reshape(-1, num_channels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for i in range(len(party_datasets)):\n",
    "    transformed = (party_datasets[i] - means) / stds\n",
    "    dataset = TensorDataset(torch.tensor(transformed), torch.tensor(party_labels[i]))\n",
    "    datasets.append(dataset)\n",
    "transformed = (candidate_dataset - means) / stds\n",
    "dataset = TensorDataset(torch.tensor(transformed), torch.tensor(candidate_labels))\n",
    "datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dataset = ConcatDataset(*datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "            concat_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iterator = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_labels = next(my_iterator)[party]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(merge(batch_images, [8, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
