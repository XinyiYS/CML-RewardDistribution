{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using disjoint classes and partitioning the dataset to 5 participants with each having 2 classes.\n",
      "participant id: 0 is getting [0 1] classes.\n",
      "participant id: 1 is getting [1 2] classes.\n",
      "participant id: 2 is getting [2 3] classes.\n",
      "participant id: 3 is getting [3 4] classes.\n",
      "participant id: 4 is getting [4 5] classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengzi/Downloads/codebase/CML-RewardDistribution/utils/utils.py:59: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729128610/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  data_indices = [(train_dataset.targets == class_id).nonzero().view(-1).tolist() for class_id in all_classes]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "train_dataset = dset.MNIST(\".data/mnist\", train=True, download=True, \n",
    "    transform=transforms.Compose([transforms.Resize(28), transforms.ToTensor(), transforms.Normalize([0.1307], [0.3081])]), )\n",
    "\n",
    "test_dataset = dset.MNIST(\".data/mnist\", train=False, download=True, \n",
    "    transform=transforms.Compose([transforms.Resize(28), transforms.ToTensor(), transforms.Normalize([0.1307], [0.3081])]), )\n",
    "\n",
    "n_participants = 5\n",
    "n_samples = 2000 * n_participants\n",
    "split_mode = 'disjointclasses'\n",
    "\n",
    "from utils.utils import split\n",
    "train_indices_list = split(n_samples, n_participants, train_dataset=train_dataset, mode=split_mode)\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loaders = [DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices)) for train_indices in train_indices_list]\n",
    "\n",
    "import itertools\n",
    "train_indices = list(itertools.chain.from_iterable(train_indices_list))\n",
    "joint_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=10000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "# for MNIST 28*28\n",
    "class MLP_MNIST(nn.Module):\n",
    "\tdef __init__(self, device=None):\n",
    "\t\tsuper(MLP_MNIST, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(784, 512)\n",
    "\t\tself.fc2 = nn.Linear(512, 128)\n",
    "\t\tself.fc3 = nn.Linear(128, 32)\n",
    "\t\tself.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.view(-1,  784)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = F.relu(self.fc3(x))\n",
    "\t\tx = self.fc4(x)\n",
    "\t\treturn x\n",
    "\t\t# return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GaussianProcessLayer(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, num_dim, grid_bounds=(-10., 10.), grid_size=64):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=grid_size, batch_shape=torch.Size([num_dim])\n",
    "        )\n",
    "        \n",
    "        # Our base variational strategy is a GridInterpolationVariationalStrategy,\n",
    "        # which places variational inducing points on a Grid\n",
    "        # We wrap it with a IndependentMultitaskVariationalStrategy so that our output is a vector-valued GP\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.GridInterpolationVariationalStrategy(\n",
    "                self, grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                variational_distribution=variational_distribution,\n",
    "            ), num_tasks=num_dim,\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.grid_bounds = grid_bounds\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "\n",
    "feature_extractor = MLP_MNIST(device=device)\n",
    "num_features = 2\n",
    "num_classes = 10\n",
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, num_dim, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GaussianProcessLayer(num_dim=num_dim, grid_bounds=grid_bounds)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_dim = num_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        # This next line makes it so that we learn a GP for each feature\n",
    "        features = features.transpose(-1, -2).unsqueeze(-1)\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "\n",
    "model = DKLModel(feature_extractor, num_dim=num_features)\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_features=model.num_dim, num_classes=num_classes)\n",
    "\n",
    "# If you run this example without CUDA, I hope you like waiting!\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "n_epochs = 20\n",
    "lr = 0.1\n",
    "optimizer = SGD([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-4},\n",
    "    {'params': model.gp_layer.hyperparameters(), 'lr': lr * 0.01},\n",
    "    {'params': model.gp_layer.variational_parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=lr, momentum=0.9, nesterov=True, weight_decay=0)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs], gamma=0.1)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(joint_loader.dataset))\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    minibatch_iter = tqdm.notebook.tqdm(joint_loader, desc=f\"(Epoch {epoch}) Minibatch\")\n",
    "    with gpytorch.settings.num_likelihood_samples(8):\n",
    "        for data, target in minibatch_iter:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = -mll(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "        \n",
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad(), gpytorch.settings.num_likelihood_samples(16):\n",
    "        for data, target in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = likelihood(model(data))  # This gives us 16 samples from the predictive distribution\n",
    "            pred = output.probs.mean(0).argmax(-1)  # Taking the mean over all of the sample we've drawn\n",
    "            correct += pred.eq(target.view_as(pred)).cpu().sum()\n",
    "    print('Test set: Accuracy: {}/{} ({}%)'.format(\n",
    "        correct, len(test_loader.dataset), 100. * correct / float(len(test_loader.dataset))\n",
    "    ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    with gpytorch.settings.use_toeplitz(False):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    scheduler.step()\n",
    "    # state_dict = model.state_dict()\n",
    "    # likelihood_state_dict = likelihood.state_dict()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90c824e11c740c7a43dfbfd860f3cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='(Epoch 20) Minibatch'), FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# need a kernel collectively defined by gpytorch and a DNN\n",
    "from utils.mmd import mmd\n",
    "\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "def repeater(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "repeated_train_loaders = [repeater(train_loader) for train_loader in train_loaders  ]\n",
    "\n",
    "\n",
    "\n",
    "def train_kernel(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    joint_minibatch_iter = tqdm.notebook.tqdm(joint_loader, desc=f\"(Epoch {epoch}) Minibatch\")\n",
    "    loaders = [joint_minibatch_iter] + repeated_train_loaders\n",
    "    with gpytorch.settings.num_likelihood_samples(8):\n",
    "        \n",
    "        for data in zip(*loaders):\n",
    "            # data is of length 6 [(data, target), (data1, target1)... (data5, target5)]\n",
    "            data = list(data)\n",
    "            data_j, target_j = data.pop(0)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                data_j, target_j = data_j.cuda(), target_j.cuda()\n",
    "                for i in range(n_participants):\n",
    "                    data[i][0], data[i][1] = data[i][0].cuda(), data[i][1].cuda()    \n",
    "            optimizer.zero_grad()            \n",
    "            loss = -torch.sum(torch.tensor([mmd(data_j.reshape(data_j.size(0), -1), \n",
    "                                               data[i][0].reshape(data[i][0].size(0), -1), k=model.gp_layer.covar_module)[0] for i in range(n_participants)],requires_grad=True))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            joint_minibatch_iter.set_postfix(loss=loss.item())\n",
    "\n",
    "train_kernel(epoch=20)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kxx, Kyy, Kxy\n",
    "\n",
    "\n",
    "e = torch.ones(m)\n",
    "m = batch_size\n",
    "(4/ falling_fac(m, 4)) * (torch.linalg.norm(torch.matmul(Kxx_, e),2) + torch.linalg.norm(torch.matmul(Kyy_, e),2) )+\n",
    "(4*(m**2 -m -1)/(m**3 *(m-1)**2)) * ((torch.linalg.norm(torch.matmul(Kxy, e),2) +(torch.linalg.norm(torch.matmul(Kxy.T, e),2) -\n",
    "(8/ (m**2 *(m**2-3m+2) ) * (torch.matmul( torch.matmul(e.T, Kxx_), torch.matmul(e, Kxy_) )  +  torch.matmul( torch.matmul(e.T, Kyy_), torch.matmul(Kxy_.T, e) ) ) )+\n",
    "                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial as fac\n",
    "def falling_fac(n, b):\n",
    "    \"\"\"\n",
    "    Return the product of n..n-b+1.\n",
    "\n",
    "    >>> falling_factorial(4, 2)  # 4*3\n",
    "    12\n",
    "    >>> falling_factorial(5, 3)  # 5*4*3\n",
    "    60\n",
    "    >>> falling_factorial(56, 1)\n",
    "    56\n",
    "    >>> falling_factorial(56, 0)\n",
    "    1\n",
    "    \"\"\"\n",
    "    r = 1  # Running product\n",
    "    for i in range(n, n-b, -1):\n",
    "        r *= i\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "factorial() only accepts integral values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4b64c3e48fd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: factorial() only accepts integral values"
     ]
    }
   ],
   "source": [
    "fac(1.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
