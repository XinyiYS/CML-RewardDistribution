{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using disjoint classes and partitioning the dataset to 5 participants with each having 2 classes.\n",
      "participant id: 0 is getting [0, 1] classes.\n",
      "participant id: 1 is getting [2, 3] classes.\n",
      "participant id: 2 is getting [4, 5] classes.\n",
      "participant id: 3 is getting [6, 7] classes.\n",
      "participant id: 4 is getting [8, 9] classes.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "train_dataset = dset.MNIST(\".data/mnist\", train=True, download=True, \n",
    "    transform=transforms.Compose([transforms.Resize(64), transforms.ToTensor(), transforms.Normalize([0.1307], [0.3081])]), )\n",
    "\n",
    "test_dataset = dset.MNIST(\".data/mnist\", train=False, download=True, \n",
    "    transform=transforms.Compose([transforms.Resize(64), transforms.ToTensor(), transforms.Normalize([0.1307], [0.3081])]), )\n",
    "\n",
    "n_participants = 5\n",
    "n_samples = 2000 * n_participants\n",
    "split_mode = 'disjointclasses'\n",
    "\n",
    "from utils.utils import split\n",
    "train_indices_list = split(n_samples, n_participants, train_dataset=train_dataset, mode=split_mode)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "class_counters = []\n",
    "for i, train_loader in enumerate(train_loaders):\n",
    "    class_counter = Counter()\n",
    "    for data, target in train_loader:\n",
    "        temp = Counter(target.tolist())\n",
    "        class_counter.update(temp)\n",
    "    class_counters.append(class_counter)\n",
    "#     print(class_counter)\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loaders = [DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices)) for train_indices in train_indices_list]\n",
    "\n",
    "import itertools\n",
    "train_indices = list(itertools.chain.from_iterable(train_indices_list))\n",
    "joint_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=10000, shuffle=True)\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "def repeater(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "repeated_train_loaders = [repeater(train_loader) for train_loader in train_loaders  ]\n",
    "\n",
    "\n",
    "from math import factorial as fac\n",
    "def falling_fac(n, b):\n",
    "    \"\"\"\n",
    "    Return the product of n..n-b+1.\n",
    "\n",
    "    >>> falling_factorial(4, 2)  # 4*3\n",
    "    12\n",
    "    >>> falling_factorial(5, 3)  # 5*4*3\n",
    "    60\n",
    "    >>> falling_factorial(56, 1)\n",
    "    56\n",
    "    >>> falling_factorial(56, 0)\n",
    "    1\n",
    "    \n",
    "    r = 1  # Running product\n",
    "    for i in range(n, n-b, -1):\n",
    "        r *= i\n",
    "    return r\n",
    "    \"\"\"\n",
    "    return fac(n) // fac(n-b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rhos(class_counters, C=10):\n",
    "    '''\n",
    "    sum of square of proportion of different class datapoints\n",
    "    '''\n",
    "    rhos = []\n",
    "    for i, class_counter in enumerate(class_counter):\n",
    "        total = sum(class_counter.values())\n",
    "        for key, value in class_counter.items():\n",
    "            rho += (1.0 * value / total)**2\n",
    "        rhos.append(rho/C)\n",
    "    return rhos\n",
    "\n",
    "def get_varrhos(rhos, class_counters):\n",
    "    '''\n",
    "    area under the curve for given rhos\n",
    "    '''\n",
    "    varrhos = []\n",
    "    for rho, class_counter in zip(rhos, class_counters):\n",
    "        n = sum(class_counter.values())\n",
    "        varrhos.append( sum([i * rho**(n-i) for i in range(n)]) )\n",
    "    return varrhos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "# for MNIST 28*28\n",
    "class MLP_MNIST(nn.Module):\n",
    "\tdef __init__(self, in_dim=784, out_dim=2, device=None):\n",
    "\t\tsuper(MLP_MNIST, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(in_dim, 512)\n",
    "\t\tself.fc2 = nn.Linear(512, 128)\n",
    "\t\tself.fc3 = nn.Linear(128, 32)\n",
    "\t\tself.fc4 = nn.Linear(32, out_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.view(-1,  784)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = F.relu(self.fc3(x))\n",
    "\t\tx = self.fc4(x)\n",
    "\t\treturn x\n",
    "\t\t# return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GaussianProcessLayer(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, num_dim, grid_bounds=(-10., 10.), grid_size=64):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=grid_size, batch_shape=torch.Size([num_dim])\n",
    "        )\n",
    "        \n",
    "        # Our base variational strategy is a GridInterpolationVariationalStrategy,\n",
    "        # which places variational inducing points on a Grid\n",
    "        # We wrap it with a IndependentMultitaskVariationalStrategy so that our output is a vector-valued GP\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.GridInterpolationVariationalStrategy(\n",
    "                self, grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                variational_distribution=variational_distribution,\n",
    "            ), num_tasks=num_dim,\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        \n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.grid_bounds = grid_bounds\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "num_features = 5\n",
    "num_classes = 10\n",
    "feature_extractor = MLP_MNIST(out_dim=num_features,device=device)\n",
    "\n",
    "def mmd(X, Y, k):\n",
    "    \"\"\"\n",
    "    Calculates unbiased MMD^2. A, B and C are the pairwise-XX, pairwise-XY, pairwise-YY summation terms respectively.\n",
    "    :param X: array of shape (n, d)\n",
    "    :param Y: array of shape (m, d)\n",
    "    :param k: GPyTorch kernel\n",
    "    :return: MMD^2, A, B, C\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    m = Y.shape[0]\n",
    "\n",
    "    X_tens = X.clone().detach().requires_grad_(True)\n",
    "    Y_tens = Y.clone().detach().requires_grad_(True)\n",
    "                \n",
    "    A = (1 / (n * (n - 1))) * (torch.sum(k(X_tens).evaluate()) - torch.sum(torch.diag(k(X_tens).evaluate())))\n",
    "    B = -(2 / (n * m)) * torch.sum(k(X_tens, Y_tens).evaluate())\n",
    "    C = (1 / (m * (m - 1))) * (torch.sum(k(Y_tens).evaluate()) - torch.sum(torch.diag(k(Y_tens).evaluate())))\n",
    "\n",
    "    Kxy  = k(X_tens, Y_tens).evaluate()\n",
    "    Kxx_ = k(X_tens, X_tens).evaluate()\n",
    "    Kxx_.fill_diagonal_(0)\n",
    "    \n",
    "    Kyy_ = k(Y_tens, Y_tens).evaluate()\n",
    "    Kyy_.fill_diagonal_(0)\n",
    "\n",
    "    return (A + B + C), Kxx_, Kxy, Kyy_\n",
    "\n",
    "\n",
    "# from utils.mmd import mmd\n",
    "\n",
    "from torch.linalg import norm\n",
    "\n",
    "def t_statistic(mmd_2, Kxx_, Kxy, Kyy_):\n",
    "    \n",
    "    \"\"\"\n",
    "    Kxy[ij] = k(X_i, Y_i)\n",
    "\n",
    "    Kxx_[ij] =  0 if i == j\n",
    "                k(X_i, X_j) o/w\n",
    "    Kyy_[ij] is similar\n",
    "\n",
    "    fro_norm = torch.linalg.norm(matrix, ord='fro')\n",
    "    \n",
    "    \"\"\"\n",
    "    m = Kxx_.size(0)\n",
    "    ex = torch.ones(Kxx_.size(0), device = Kxx_.device)\n",
    "    ey = torch.ones(Kyy_.size(0), device = Kyy_.device)\n",
    "\n",
    "    vhat = 0\n",
    "\n",
    "    #1st term\n",
    "    constant = 4 / falling_fac(m, 4)\n",
    "    a = torch.square(norm(Kxx_ @ ex)) + torch.square(norm(Kyy_ @ ey))\n",
    "    vhat += constant * a\n",
    "    \n",
    "    #2nd term\n",
    "    constant = 4*(m**2 - m - 1) / (m**3 * (m - 1)**2)\n",
    "    a = torch.square(norm(Kxy @ ey)) + torch.square(norm(Kxy.T @ ex))\n",
    "    vhat += constant * a\n",
    "\n",
    "    # 3rd term\n",
    "    constant = - 8/ (m**2 * (m**2 - 3 * m + 2))   \n",
    "    a =  ex.T @ Kxx_ @ Kxy @ ey + ey.T @ Kyy_ @ Kxy.T @ ex\n",
    "    vhat += constant * a\n",
    "\n",
    "    # 4th term\n",
    "    constant = 8 / (m**2 * falling_fac(m, 3))\n",
    "    a = (ex.T @ Kxx_ @ ex + ey.T @ Kyy_ @ ey) * (ex.T @ Kxy @ ey) \n",
    "    vhat += constant * a\n",
    "\n",
    "    #5th term\n",
    "    constant = - 2*(2*m -3)/(falling_fac(m, 2) * falling_fac(m, 4))\n",
    "    a = torch.square(ex.T @ Kxx_ @ ex) + torch.square(ey.T @ Kyy_ @ ey)\n",
    "    vhat += constant * a\n",
    "\n",
    "    #6th term\n",
    "    constant = -4 * (2*m - 3) / (m**3 * (m - 1)**3)\n",
    "    a = torch.square(ex.T @ Kxy @ ey)\n",
    "    vhat += constant * a\n",
    "\n",
    "    #7th term\n",
    "    constant = - 2/ (m* ( m**3 - 6 * m**2 + 11*m - 6 ))\n",
    "    a = torch.square(norm(Kxx_, ord='fro')) + torch.square(norm(Kyy_, ord='fro'))\n",
    "    vhat += constant * a\n",
    "\n",
    "    #8th term\n",
    "    constant = 4 * (m-2) / (m**2 *(m-1)**3)\n",
    "    a = torch.square(norm(Kxy, ord='fro'))\n",
    "    vhat += constant * a\n",
    "        \n",
    "    if vhat < 0:\n",
    "        print('vhat is negative:', vhat.item())\n",
    "        print('this leads to NaN at t_stat')\n",
    "\n",
    "    return torch.div(mmd_2, torch.sqrt(vhat))\n",
    "\n",
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, num_dim, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GaussianProcessLayer(num_dim=num_dim, grid_bounds=grid_bounds)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_dim = num_dim\n",
    "    \n",
    "    def forward(self, x1, x2):        \n",
    "        features1 = self.get_features(x1)\n",
    "        features2 = self.get_features(x2)        \n",
    "        mmd_2, Kxx_, Kxy, Kyy_ = mmd(features1.reshape(len(x1), -1), features2.reshape(len(x2), -1), k=self.gp_layer.covar_module)\n",
    "        t_stat = t_statistic(mmd_2, Kxx_, Kxy, Kyy_)        \n",
    "        return t_stat, self.gp_layer(features1), self.gp_layer(features2)\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        # This next line makes it so that we learn a GP for each feature\n",
    "        features = features.transpose(-1, -2).unsqueeze(-1)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a kernel collectively defined by gpytorch and a DNN\n",
    "def train_kernel(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    joint_minibatch_iter = tqdm.notebook.tqdm(joint_loader, desc=f\"(Epoch {epoch}) Minibatch\")\n",
    "    loaders = [joint_minibatch_iter] + repeated_train_loaders\n",
    "    with gpytorch.settings.num_likelihood_samples(8):\n",
    "        \n",
    "        for data in zip(*loaders):\n",
    "            # data is of length 6 [(data, target), (data1, target1)... (data5, target5)]\n",
    "            data = list(data)\n",
    "            data_j, target_j = data.pop(0)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                data_j, target_j = data_j.cuda(), target_j.cuda()\n",
    "                for i in range(n_participants):\n",
    "                    data[i][0], data[i][1] = data[i][0].cuda(), data[i][1].cuda()    \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            mmd_loss, likelihood_loss = 0, 0\n",
    "            for i in range(n_participants):\n",
    "                t_stat, res_j, res_i = model(data_j, data[i][0])\n",
    "                mmd_loss += -t_stat\n",
    "                if torch.isnan(t_stat):\n",
    "                    print(\"got nan value, t_stat is nan\")\n",
    "                    return\n",
    "            \n",
    "                if i == 0:\n",
    "                    # increment the likelihood loss for joint data only once\n",
    "                    likelihood_loss += -mll(res_j, target_j)\n",
    "    \n",
    "                likelihood_loss += -mll(res_i, data[i][1])\n",
    "            loss = mmd_loss + likelihood_loss\n",
    "            \n",
    "            '''\n",
    "            for mmd_loss, need to consider over the entire data_loader and not the batches\n",
    "            to do so, use the mmd_update method\n",
    "            \n",
    "            as a result, if we ignore likelihood_loss altogether,\n",
    "            we can incrementally update the loss over the data_loader and only do step() \n",
    "            at the last batch\n",
    "            \n",
    "            however, if we wish to include the likelihood_loss, we need to more carefully design the loss function\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            joint_minibatch_iter.set_postfix(loss=loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):        \n",
    "    train_kernel(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DKLModel(feature_extractor, num_dim=num_features)\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_features=model.num_dim, num_classes=num_classes)\n",
    "\n",
    "# If you run this example without CUDA, I hope you like waiting!\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "n_epochs = 20\n",
    "lr = 0.1\n",
    "optimizer = SGD([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-4},\n",
    "    {'params': model.gp_layer.hyperparameters(), 'lr': lr * 0.01},\n",
    "    {'params': model.gp_layer.variational_parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=lr, momentum=0.9, nesterov=True, weight_decay=0)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs], gamma=0.1)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(joint_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngpu = 1\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 1\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# load the models\n",
    "# from models.Conditional_DCGAN_MNIST import Discriminator, Generator\n",
    "\n",
    "def train_individual_gan(data_loader, ngpu=1, nz=100, nclass=10, nepochs=50, lr=0.0002, beta1=0.5, device=None):\n",
    "    netG = Generator(ngpu=1).to(device)\n",
    "    netD = Discriminator(ngpu=1).to(device)\n",
    "    \n",
    "    netG.apply(weights_init)\n",
    "    netD.apply(weights_init)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    real_label, fake_label = 1, 0\n",
    "\n",
    "    # setup optimizer\n",
    "    optimizerD = Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerG = Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "    epochs = range(nepochs)\n",
    "    epochs = tqdm.notebook.tqdm(epochs, desc=f\" Epoch\")\n",
    "    for epoch in epochs:\n",
    "        for i, (data, target) in enumerate(data_loader, 0):\n",
    "\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            # train with real\n",
    "            netD.zero_grad()\n",
    "            real_data = data.to(device)\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "#             digit_target = torch.nn.functional.one_hot(target, nclass)\n",
    "#             digit_target = digit_target.unsqueeze(2).unsqueeze(3)\n",
    "#             digit_target = digit_target.expand([batch_size, digit_target.size(1), 28, 28]).to(device)\n",
    "            \n",
    "            label = torch.full((batch_size,), real_label, dtype=real_data.dtype, device=device)\n",
    "#             output = netD(real_data, digit_target)\n",
    "            output = netD(real_data)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "\n",
    "            # train with fake\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            G_target = torch.nn.functional.one_hot(target, nclass)\n",
    "            G_target = G_target.unsqueeze(2).unsqueeze(3).to(device)\n",
    "#             fake = netG(noise, G_target)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "#             output = netD(fake.detach(), digit_target)\n",
    "            output = netD(fake.detach())\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "#             output = netD(fake, digit_target)\n",
    "            output = netD(fake)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "#         print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' \n",
    "#             % (epoch+1, nepochs, i+1, len(data_loader),\n",
    "#                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "    return netG, netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = []\n",
    "for i, train_loader in enumerate(train_loaders):\n",
    "    print(\"{}-participant\".format(i+1), end='  ')\n",
    "    G, D =  train_individual_gan(train_loader, device=device, lr=0.001, nclass=10, nepochs=50)\n",
    "    Gs.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_number = 10\n",
    "latent_size = 100\n",
    "\n",
    "batch_size = 10\n",
    "fixed_noise = torch.randn(batch_size, latent_size, 1, 1)\n",
    "\n",
    "for i, G in enumerate(Gs):\n",
    "    labels = torch.tensor( [i, i+1] * (batch_size//2 ),   dtype=int).reshape(batch_size)\n",
    "#     G_target = torch.nn.functional.one_hot(labels, class_number)\n",
    "#     G_target = G_target.unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "    fixed_noise = torch.randn(batch_size, latent_size, 1, 1)\n",
    "    if torch.cuda.is_available():\n",
    "        G = G.cuda()\n",
    "        fixed_noise = fixed_noise.cuda()\n",
    "#         G_target = G_target.cuda()\n",
    "    fake_images = G(fixed_noise)\n",
    "#     fake_images = G(fixed_noise, G_target)\n",
    "\n",
    "    fake_images_np = fake_images.cpu().detach().numpy()\n",
    "    fake_images_np = fake_images_np.reshape(fake_images_np.shape[0], 28, 28)\n",
    "    R, C = 5, 2\n",
    "    for i in range(batch_size):\n",
    "        plt.subplot(R, C, i + 1)\n",
    "        plt.imshow(fake_images_np[i], cmap='gray')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    minibatch_iter = tqdm.notebook.tqdm(joint_loader, desc=f\"(Epoch {epoch}) Minibatch\")\n",
    "    with gpytorch.settings.num_likelihood_samples(8):\n",
    "        for data, target in minibatch_iter:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            _,output, _ = model(data, data)\n",
    "            print('output:', output)\n",
    "            print('target:',target)\n",
    "            loss = -mll(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "        \n",
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad(), gpytorch.settings.num_likelihood_samples(16):\n",
    "        for data, target in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = likelihood(model(data))  # This gives us 16 samples from the predictive distribution\n",
    "            pred = output.probs.mean(0).argmax(-1)  # Taking the mean over all of the sample we've drawn\n",
    "            correct += pred.eq(target.view_as(pred)).cpu().sum()\n",
    "    print('Test set: Accuracy: {}/{} ({}%)'.format(\n",
    "        correct, len(test_loader.dataset), 100. * correct / float(len(test_loader.dataset))\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    with gpytorch.settings.use_toeplitz(False):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    scheduler.step()\n",
    "    # state_dict = model.state_dict()\n",
    "    # likelihood_state_dict = likelihood.state_dict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
