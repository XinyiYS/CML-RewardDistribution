{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join as oj\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "import gpytorch\n",
    "from tqdm.notebook import trange\n",
    "import heapq\n",
    "import math\n",
    "import pickle\n",
    "from algorithms.cd import con_div\n",
    "from algorithms.ccr import con_conv_rate\n",
    "from utils.class_imbalance import get_classes, class_proportion\n",
    "from utils.mmd import mmd, perm_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from utils.utils import tabulate_dict, prepare_loaders, evaluate, init_deterministic, load_dataset\n",
    "from run import construct_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_to_use = 1\n",
    "\n",
    "# # setting device on GPU if available, else CPU\n",
    "# device = torch.device('cuda:{}'.format(gpu_to_use) if torch.cuda.is_available() else 'cpu')\n",
    "# print('Using device:', device)\n",
    "# print()\n",
    "\n",
    "# #Additional Info when using cuda\n",
    "# if device.type == 'cuda':\n",
    "#     print(torch.cuda.get_device_name(0))\n",
    "#     print('Memory Usage:')\n",
    "#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#     print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameters and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiment dir\n",
    "load_dir = 'MNIST/N2000-E30-B832'\n",
    "\n",
    "\n",
    "# Read the kernel architecture hyperparameters\n",
    "args = {} \n",
    "with open(oj(load_dir,'settings_dict.txt') ,'r') as file:\n",
    "    for line in file.readlines():\n",
    "        (key, value) = line.strip().split(' : ', 1)\n",
    "\n",
    "        try: \n",
    "            args[key] = literal_eval(value)\n",
    "        except Exception as e:\n",
    "            args[key] = value\n",
    "init_deterministic(args['noise_seed']) # comment this out for faster runtime\n",
    "\n",
    "\n",
    "# Initialize the kernel, including initializing and loading pretrained weights for the shared feature extrator \n",
    "kernel, _ = construct_kernel(args)\n",
    "kernel.eval()\n",
    "\n",
    "# Load pretrained weights: including the individual MLP layers and the Gpytorch Hyperparameters\n",
    "trained_kernel_dir = oj(load_dir, 'trained_kernels', 'model_-E25.pth')\n",
    "kernel.load_state_dict(torch.load(trained_kernel_dir), strict=False)\n",
    "\n",
    "\n",
    "# Construct data loaders for a quick evaluation\n",
    "joint_loader, train_loaders, joint_test_loader, test_loaders = prepare_loaders(args, repeat=False)\n",
    "\n",
    "test_logs_dir = oj(\"test_logs_dir\", args['dataset'])\n",
    "os.makedirs(test_logs_dir, exist_ok=True)\n",
    "if args['include_joint']:\n",
    "    train_loaders = [joint_loader] + train_loaders\n",
    "    test_loaders = [joint_test_loader] + test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_parties = 6  # 5 + joint\n",
    "\n",
    "all_party_datasets = []\n",
    "all_party_labels = []\n",
    "for i in range(num_parties):\n",
    "    iterator = iter(train_loaders[i])\n",
    "    party_images = []\n",
    "    party_labels = []\n",
    "    while True:\n",
    "        try:\n",
    "            images, labels = next(iterator)\n",
    "            party_images.append(images.cpu().numpy())\n",
    "            party_labels.append(labels.cpu().numpy())\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "    party_dataset = np.concatenate(party_images)\n",
    "    party_labels = np.concatenate(party_labels)\n",
    "    all_party_datasets.append(party_dataset)\n",
    "    all_party_labels.append(party_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "party = 0\n",
    "i = 21\n",
    "\n",
    "plt.imshow(np.transpose(all_party_datasets[party][i], [1, 2, 0]))\n",
    "all_party_labels[party][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_test_datasets = []\n",
    "all_test_labels = []\n",
    "for i in range(1, num_parties):\n",
    "    iterator = iter(test_loaders[i])\n",
    "    party_images = []\n",
    "    party_labels = []\n",
    "    while True:\n",
    "        try:\n",
    "            images, labels = next(iterator)\n",
    "            party_images.append(images.cpu().numpy())\n",
    "            party_labels.append(labels.cpu().numpy())\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "    party_dataset = np.concatenate(party_images)\n",
    "    party_labels = np.concatenate(party_labels)\n",
    "    all_test_datasets.append(party_dataset)\n",
    "    all_test_labels.append(party_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "party = 2\n",
    "i = 219\n",
    "\n",
    "plt.imshow(np.transpose(all_test_datasets[party][i], [1, 2, 0]))\n",
    "all_test_labels[party][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X, model):\n",
    "    with torch.no_grad():\n",
    "        X_tens = torch.tensor(X).cuda()\n",
    "        X_feat = kernel.indi_feature_extractors(model.get_vae_features(X_tens))\n",
    "    return X_feat.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything to features\n",
    "all_party_feats = [get_features(X, kernel) for X in all_party_datasets]\n",
    "all_test_feats = [get_features(X, kernel) for X in all_test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_wrapper(kernel):\n",
    "    \"\"\"\n",
    "    But I'm not a rapper.\n",
    "    \"\"\"\n",
    "    def wrapper(X, Y=None):\n",
    "        with torch.no_grad():\n",
    "            X_tens = X.clone().detach().cuda()\n",
    "            k = kernel.gp_layer.covar_module\n",
    "            X_feat = kernel.indi_feature_extractors(kernel.get_vae_features(X_tens))\n",
    "            \n",
    "            if Y is None:\n",
    "                retval = k(X_feat, X_feat)\n",
    "            else:\n",
    "                Y_tens = Y.clone().detach().cuda()\n",
    "                Y_feat = kernel.indi_feature_extractors(kernel.get_vae_features(Y_tens))\n",
    "                retval = k(X_feat, Y_feat)\n",
    "                del Y_tens\n",
    "                \n",
    "            del X_tens\n",
    "            torch.cuda.empty_cache()\n",
    "            return retval\n",
    "            \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "k.base_kernel.lengthscale = kernel.gp_layer.covar_module.base_kernel.kernels[0].lengthscale\n",
    "# k.base_kernel.lengthscale_prior = kernel.gp_layer.covar_module.base_kernel.kernels[0].lengthscale_prior\n",
    "# k.base_kernel.distance_module = kernel.gp_layer.covar_module.base_kernel.kernels[0].distance_module\n",
    "k.outputscale = kernel.gp_layer.covar_module.outputscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(num_parties):\n",
    "#     print(\"Party {}\".format(i))\n",
    "#     for j in range(num_parties):\n",
    "#         print(\"With party {}: {}\".format(j, mmd(all_party_feats[i], all_party_feats[j], k)[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "# Use test dataset as reference and candidates\n",
    "ref_cand_dataset, ref_cand_labels = unison_shuffled_copies(np.concatenate(all_test_datasets), \n",
    "                                                           np.concatenate(all_test_labels))\n",
    "ref_cand_feats = get_features(ref_cand_dataset, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_exps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reference = len(all_party_datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dataset = ref_cand_feats[:num_reference]\n",
    "ref_labels = ref_cand_labels[:num_reference]\n",
    "cand_dataset = ref_cand_feats[num_reference:]\n",
    "cand_labels = ref_cand_labels[num_reference:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_dataset = np.tile(cand_dataset, (num_exps, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = np.linspace(0.1, 1, num_exps)\n",
    "greeds = np.ones(num_exps) * 4\n",
    "eta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_all_res = []\n",
    "cd_all_deltas = []\n",
    "cd_all_mus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, num_parties):\n",
    "    D = np.array([all_party_feats[i]] * num_exps)\n",
    "    res, deltas, mus = con_div(candidates=cand_dataset, \n",
    "                               Y=ref_dataset, \n",
    "                               phi=phi, \n",
    "                               D=D, \n",
    "                               kernel=k,\n",
    "                               perm_samp_dataset=np.concatenate(all_party_feats[1:]),\n",
    "                               num_perms=200, \n",
    "                               greeds=greeds, \n",
    "                               eta=eta)\n",
    "    cd_all_res.append(res)\n",
    "    cd_all_deltas.append(deltas)\n",
    "    cd_all_mus.append(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((cd_all_res, cd_all_deltas, cd_all_mus), open(\"experiments/results/MNIST-CD-allclusters.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_all_res, cd_all_deltas, cd_all_mus = pickle.load(open(\"experiments/results/MNIST-CD-allclusters.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_parties-1):\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    plt.plot(phi, [len(result) for result in cd_all_res[i]])\n",
    "    plt.xlabel(\"$\\phi$\")\n",
    "    plt.ylabel(\"Number of points added\")\n",
    "    plt.title(\"Party {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_props = []\n",
    "all_bad_props = []\n",
    "for i in range(num_parties-1):\n",
    "    class_props = []\n",
    "    bad_props = []\n",
    "    res = cd_all_res[i]\n",
    "    \n",
    "    existing_classes = all_party_labels[i+1]\n",
    "    \n",
    "    for result in res:\n",
    "        class_props.append(class_proportion(np.concatenate([get_classes(np.array(result), cand_dataset[0], cand_labels),\n",
    "                                                            existing_classes]), num_clusters))\n",
    "        bad_props.append(class_proportion(np.concatenate([list(np.random.randint(0, num_clusters, len(result))),\n",
    "                                                         existing_classes]), num_clusters))\n",
    "    all_class_props.append(class_props)\n",
    "    all_bad_props.append(bad_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams['mathtext.fontset'] = 'dejavuserif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(num_parties-1):\n",
    "    plt.figure(figsize=(3, 3), dpi=300)\n",
    "    plt.plot(phi, [prop[1] for prop in all_class_props[i]], label=\"CD\", color=cm.get_cmap('Spectral')(0.9))\n",
    "    plt.plot(phi, [prop[1] for prop in all_bad_props[i]], label=\"Random sampling\", color=cm.get_cmap('Spectral')(0.1))\n",
    "    plt.xlabel(\"$\\phi$\", fontsize=16)\n",
    "    plt.ylabel(\"$\\\\rho$\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.title(\"Party {} (MNIST digits {}, {})\".format(i+1, i*2, i*2+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_added_props = []\n",
    "for i in range(num_parties-1):\n",
    "    class_props = []\n",
    "    res = cd_all_res[i]    \n",
    "    for result in res:\n",
    "        class_props.append(class_proportion(get_classes(np.array(result), cand_dataset[0], cand_labels), num_clusters))\n",
    "        bad_props.append(class_proportion(list(np.random.randint(0, num_clusters, len(result))), num_clusters))\n",
    "    all_added_props.append(class_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_added_props[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corrcoef = []\n",
    "for i in range(num_parties-1):\n",
    "    class_props = all_class_props[i]\n",
    "    props = [pair[1] for pair in class_props]\n",
    "    all_corrcoef.append(np.corrcoef(np.array(list(zip(phi, props))).T)[0,1])\n",
    "print(\"Average correlation coefficient: {}\".format(np.mean(all_corrcoef)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_exps = 10\n",
    "num_reference = len(all_party_datasets[1])\n",
    "phi = np.linspace(0.1, 1, num_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dataset = ref_cand_feats[:num_reference]\n",
    "ref_labels = ref_cand_labels[:num_reference]\n",
    "cand_dataset = ref_cand_feats[num_reference:]\n",
    "cand_labels = ref_cand_labels[num_reference:]\n",
    "cand_dataset = np.tile(cand_dataset, (num_exps, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccr_all_res = []\n",
    "ccr_all_deltas = []\n",
    "ccr_all_mus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, num_parties):\n",
    "    D = np.array([all_party_feats[i]] * num_exps)\n",
    "    res, deltas, mus = con_conv_rate(candidates=cand_dataset, \n",
    "                                     Y=ref_dataset, \n",
    "                                     phi=phi, \n",
    "                                     D=D, \n",
    "                                     kernel=k)\n",
    "    ccr_all_res.append(res)\n",
    "    ccr_all_deltas.append(deltas)\n",
    "    ccr_all_mus.append(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((ccr_all_res, ccr_all_deltas, ccr_all_mus), open(\"experiments/results/MNIST-CCR-allclusters.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ccr_all_res, ccr_all_deltas, ccr_all_mus) = pickle.load(open(\"experiments/results/MNIST-CCR-allclusters.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j in range(num_parties-1):\n",
    "    mus = ccr_all_mus[j]\n",
    "    x = list(range(1, len(mus[0])+1))\n",
    "    plt.figure(figsize=(3, 3), dpi=300)\n",
    "    phi_labels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    for i in range(len(mus)):\n",
    "        if int(phi[i]*10) % 2 == 0:\n",
    "            plt.plot(x, mus[i], 'C0', linewidth=1, color=cm.get_cmap('Spectral')(phi[i]), label=\"$\\phi = ${}\".format(phi_labels[i]))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Party {} (MNIST digits {}, {})\".format(j+1, j*2, j*2+1))\n",
    "    plt.ylabel(\"$z(D \\cup R_i)$\", fontsize=16)\n",
    "    plt.xlabel(\"$|R_i|$\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Es = []\n",
    "all_class_prop_AUCs = []\n",
    "all_corrcoeff = []\n",
    "\n",
    "for party in range(num_parties-1):\n",
    "    R = ccr_all_res[party]\n",
    "    class_props = [[] for i in range(num_exps)]\n",
    "    num_candidate_points = cand_dataset.shape[1]\n",
    "    deltas = ccr_all_deltas[party]\n",
    "    \n",
    "    for i in range(num_exps):\n",
    "        reward_set = R[i]\n",
    "        classes = get_classes(np.array(reward_set), cand_dataset[0], cand_labels)\n",
    "        for j in range(num_candidate_points):\n",
    "            current_classes = np.concatenate((classes[:j+1], all_party_labels[party+1]))\n",
    "            class_props[i].append(class_proportion(current_classes, num_clusters)[1])\n",
    "    \n",
    "    Es = []\n",
    "    class_prop_AUCs = []\n",
    "    for i in range(num_exps):\n",
    "        delta = np.array(deltas[i])\n",
    "        Es.append(np.sum(delta[:-1] * np.arange(num_candidate_points-1, 0, -1)))\n",
    "        props = np.array(class_props[i])\n",
    "        class_prop_AUCs.append(np.sum(props[:-1] * np.arange(num_candidate_points-1, 0, -1)))\n",
    "    \n",
    "    all_Es.append(Es)\n",
    "    all_class_prop_AUCs.append(class_prop_AUCs)\n",
    "    \n",
    "    all_corrcoeff.append(np.corrcoef(np.array(list(zip(Es, class_prop_AUCs))).T)[0, 1])\n",
    "\n",
    "print(\"Average correlation coefficient: {}\".format(np.mean(all_corrcoeff)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
