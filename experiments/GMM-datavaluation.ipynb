{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "import gpytorch\n",
    "from tqdm.notebook import trange\n",
    "import heapq\n",
    "import math\n",
    "import pickle\n",
    "from algorithms.cd import con_div\n",
    "from algorithms.ccr import con_conv_rate\n",
    "from utils.class_imbalance import get_classes, class_proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_GMM(means, covs, num_samples):\n",
    "    \"\"\"\n",
    "    Samples equally from clusters of normal distributions.\n",
    "    \"\"\"\n",
    "    assert(means.shape[0] == covs.shape[0])\n",
    "    assert(means.shape[1] == covs.shape[1])\n",
    "    assert(covs.shape[1] == covs.shape[2])\n",
    "    \n",
    "    n = means.shape[0]\n",
    "    d = means.shape[1]\n",
    "    samples = np.zeros((num_samples, d))\n",
    "    clusters = np.zeros(num_samples, dtype=np.int32)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        cluster = np.random.randint(n)\n",
    "        samples[i] = np.random.multivariate_normal(means[cluster], covs[cluster], check_valid='raise')\n",
    "        clusters[i] = cluster\n",
    "    \n",
    "    return samples, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "d = 2\n",
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means = np.random.uniform(size=(num_clusters, d))\n",
    "covs = np.zeros((num_clusters, d, d))\n",
    "for i in range(num_clusters):\n",
    "    covs[i] = np.eye(d)/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sets = np.zeros((num_clusters, num_samples, d))\n",
    "test_sets = np.zeros((num_clusters, num_samples, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    train_sets[i] = np.random.multivariate_normal(means[i], covs[i], size=(num_samples), check_valid='raise')\n",
    "    test_sets[i] = np.random.multivariate_normal(means[i], covs[i], size=(num_samples), check_valid='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"sans-serif\"})\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "#plt.gca().set_aspect('equal', adjustable='box')\n",
    "for i in range(num_clusters):\n",
    "    plt.scatter(train_sets[i, :, 0], train_sets[i, :, 1], s=2, color=cm.get_cmap('Set1')(i*(1/9)), label=\"{0}\".format(i))\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data valuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.mmd import mmd\n",
    "from sympy.utilities.iterables import multiset_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=d))\n",
    "kernel.base_kernel.lengthscale = [1, 1]\n",
    "kernel.outputscale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def shapley_mmd(parties_datasets, reference_dataset, kernel):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    num_parties = len(parties_datasets)\n",
    "    shapley_sums = np.zeros(num_parties)\n",
    "    \n",
    "    perms = multiset_permutations([i for i in range(num_parties)])\n",
    "    \n",
    "    for perm in perms:\n",
    "        print(perm)\n",
    "        current_neg_mmd = 0\n",
    "        for i in range(num_parties):\n",
    "            party = perm[i]\n",
    "            if i == 0:\n",
    "                current_dataset = parties_datasets[party]\n",
    "            else:\n",
    "                current_dataset = np.concatenate([current_dataset, parties_datasets[party]])\n",
    "           \n",
    "            prev_neg_mmd = current_neg_mmd\n",
    "            current_neg_mmd = -mmd(current_dataset, reference_dataset, kernel)[0]\n",
    "            diff = current_neg_mmd - prev_neg_mmd\n",
    "            shapley_sums[party] += diff\n",
    "    \n",
    "    return (1/math.factorial(num_parties)) * shapley_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reference_datasets, reference_labels = sample_GMM(means, covs, num_samples * num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap = shapley_mmd(test_sets, reference_datasets, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proportions = np.array([[0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "                        [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "                        [0.6, 0.1, 0.1, 0.1, 0.1],\n",
    "                        [0.0, 0.5, 0.5, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_proportions(dataset, proportions):\n",
    "    \"\"\"\n",
    "    :param dataset: array of shape (num_classes, N, d).\n",
    "    :param proportions: array of probability simplices of shape (num_classes, num_classes). Must sum to 1 along\n",
    "    all rows and columns\n",
    "    \"\"\"\n",
    "    num_classes, N, d = dataset.shape\n",
    "    split_datasets = [[] for i in range(num_classes)]\n",
    "    dataset_idx = [0 for i in range(num_classes)]\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            prop = proportions[i, j]\n",
    "            for k in range(int(prop * N)):\n",
    "                split_datasets[i].append(dataset[j, dataset_idx[j]])\n",
    "                dataset_idx[j] += 1\n",
    "    \n",
    "    return np.array(split_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_datasets = split_proportions(test_sets, proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap = shapley_mmd(split_datasets, reference_datasets, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sum(shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-mmd(reference_datasets, reference_datasets, kernel)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ds in split_datasets:\n",
    "    print(-mmd(ds, reference_datasets, kernel)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-mmd(np.concatenate([split_datasets[0], split_datasets[1]]), reference_datasets, kernel)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled divergence (CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_candidate_points = 10000\n",
    "num_parties = 10\n",
    "\n",
    "phi = np.linspace(0.05, 1, num_parties)\n",
    "\n",
    "gmm_clusters = [sample_GMM(means, covs, num_candidate_points) for i in range(num_clusters)]\n",
    "gmm = np.array([pair[0] for pair in gmm_clusters])\n",
    "clusters = np.array([pair[1] for pair in gmm_clusters])\n",
    "\n",
    "reference = sample_GMM(means, covs, num_samples)[0]\n",
    "candidates = np.array([gmm[0]]*num_parties)\n",
    "\n",
    "greeds = np.ones(num_parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=d))\n",
    "kernel.base_kernel.lengthscale = [1, 1]\n",
    "kernel.outputscale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd_all_res = []\n",
    "cd_all_deltas = []\n",
    "cd_all_mus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eta = 100/((len(candidates) + len(reference))/2) \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    D = np.array([test_sets[i]] * num_parties)\n",
    "    res, deltas, mus = con_div(candidates, reference, phi, D, kernel, num_perms=1000, greeds=greeds, eta=eta)\n",
    "    cd_all_res.append(res)\n",
    "    cd_all_deltas.append(deltas)\n",
    "    cd_all_mus.append(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((gmm, clusters, reference, candidates, test_sets, greeds, cd_all_res, cd_all_deltas, cd_all_mus), open(\"CDWS-allclusters10000cands.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(gmm, clusters, reference, candidates, test_sets, greeds, cd_all_res, cd_all_deltas, cd_all_mus) = pickle.load(open(\"results/CDWS-allclusters-10000cands.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    plt.plot(phi, [len(result) for result in cd_all_res[i]])\n",
    "    plt.xlabel(\"$\\phi$\")\n",
    "    plt.ylabel(\"Number of points added\")\n",
    "    plt.title(\"Cluster {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_class_props = []\n",
    "all_bad_props = []\n",
    "for i in range(num_clusters):\n",
    "    class_props = []\n",
    "    bad_props = []\n",
    "    res = cd_all_res[i]\n",
    "    for result in res:\n",
    "        class_props.append(class_proportion(get_classes(np.array(result), gmm[0], clusters[0]) + \n",
    "                                            [i for _ in range(len(test_sets[i]))], num_clusters))\n",
    "        bad_props.append(class_proportion([i for _ in range(len(test_sets[i]))] + \n",
    "                                          list(np.random.randint(0, num_clusters, len(result))), num_clusters))\n",
    "    all_class_props.append(class_props)\n",
    "    all_bad_props.append(bad_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams['mathtext.fontset'] = 'dejavuserif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phi = np.linspace(0.05, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    plt.figure(figsize=(3, 3), dpi=300)\n",
    "    plt.plot(phi, [prop[1] for prop in all_class_props[i]], label=\"CD\", color=cm.get_cmap('Spectral')(0.9))\n",
    "    plt.plot(phi, [prop[1] for prop in all_bad_props[i]], label=\"Random sampling\", color=cm.get_cmap('Spectral')(0.1))\n",
    "    plt.xlabel(\"$\\phi$\", fontsize=16)\n",
    "    plt.ylabel(\"$\\\\rho$\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.title(\"Party {} (GMM cluster {})\".format(i+1, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_corrcoef = []\n",
    "for i in range(num_clusters):\n",
    "    class_props = all_class_props[i]\n",
    "    props = [pair[1] for pair in class_props]\n",
    "    all_corrcoef.append(np.corrcoef(np.array(list(zip(phi, props))).T)[0,1])\n",
    "print(\"Average correlation coefficient: {}\".format(np.mean(all_corrcoef)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cluster in range(num_clusters):\n",
    "    res = cd_all_res[cluster]\n",
    "    party = 9  # Look at highest reward\n",
    "    \n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    for i in range(num_clusters):\n",
    "        if i != party:\n",
    "            plt.scatter(test_sets[i, :, 0], test_sets[i, :, 1], s=0.1, color='grey')\n",
    "    plt.scatter(test_sets[cluster, :, 0], test_sets[cluster, :, 1], s=10, color=cm.get_cmap('Set1')(0*(1/9)), label=\"Party {}\".format(party))\n",
    "\n",
    "    added = np.array(res[party])\n",
    "    alphas = [1-i*(1/len(added)) for i in range(len(added))]\n",
    "    rgba_colors = np.zeros((len(added),4))\n",
    "    rgba_colors[:, 3] = alphas\n",
    "    rgba_colors[:, :3] = (0.21568627450980393, 0.49411764705882355, 0.7215686274509804)\n",
    "    plt.scatter(added[:, 0], added[:, 1], s=10, color=rgba_colors, label=\"Added\")\n",
    "    \n",
    "    plt.xlabel(\"$x_0$\")\n",
    "    plt.ylabel(\"$x_1$\")\n",
    "    \n",
    "    plt.title(\"Party {}, $\\phi = 1.0$\".format(cluster))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlled convergence rate (CCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_candidate_points = 2000\n",
    "num_parties = 10\n",
    "\n",
    "gmm_clusters = [sample_GMM(means, covs, num_candidate_points) for i in range(num_clusters)]\n",
    "gmm = np.array([pair[0] for pair in gmm_clusters])\n",
    "clusters = np.array([pair[1] for pair in gmm_clusters])\n",
    "\n",
    "reference = sample_GMM(means, covs, num_samples)[0]\n",
    "candidates = np.array([gmm[0]]*num_parties)\n",
    "phi = np.linspace(0.1, 1, num_parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=d))\n",
    "kernel.base_kernel.lengthscale = [1, 1]\n",
    "kernel.outputscale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ccr_all_res = []\n",
    "ccr_all_deltas = []\n",
    "ccr_all_mus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    D = np.array([test_sets[i]] * num_parties)\n",
    "    res, deltas, mus = con_conv_rate(candidates, reference, phi, D, kernel)\n",
    "    ccr_all_res.append(res)\n",
    "    ccr_all_deltas.append(deltas)\n",
    "    ccr_all_mus.append(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((gmm, clusters, reference, candidates, phi, test_sets, ccr_all_res, ccr_all_deltas, ccr_all_mus), open(\"CCR-allclusters.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(gmm, clusters, reference, candidates, phi, test_sets, ccr_all_res, ccr_all_deltas, ccr_all_mus) = pickle.load(open(\"results/CCR-allclusters.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(num_clusters):\n",
    "    mus = ccr_all_mus[j]\n",
    "    x = list(range(1, len(mus[0])+1))\n",
    "    plt.figure(figsize=(3, 3), dpi=300)\n",
    "    phi_labels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    for i in range(len(mus)):\n",
    "        if int(phi[i]*10) % 2 == 0:\n",
    "            plt.plot(x, mus[i], 'C0', linewidth=1, color=cm.get_cmap('Spectral')(phi[i]), label=\"$\\phi = ${}\".format(phi_labels[i]))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Party {} (GMM cluster {})\".format(j+1, j+1))\n",
    "    plt.ylabel(\"$z(D \\cup R_i)$\", fontsize=16)\n",
    "    plt.xlabel(\"$|R_i|$\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_Es = []\n",
    "all_class_prop_AUCs = []\n",
    "all_corrcoeff = []\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    R = ccr_all_res[cluster]\n",
    "    class_props = [[] for i in range(num_parties)]\n",
    "    num_candidate_points = candidates.shape[1]\n",
    "    deltas = ccr_all_deltas[cluster]\n",
    "    \n",
    "    for i in range(num_parties):\n",
    "        reward_set = R[i]\n",
    "        classes = get_classes(np.array(reward_set), gmm[0], clusters[0])\n",
    "        for j in range(num_candidate_points):\n",
    "            current_classes = classes[:j+1] + [cluster for k in range(len(test_sets[cluster]))]\n",
    "            class_props[i].append(class_proportion(current_classes, num_clusters)[1])\n",
    "    \n",
    "    Es = []\n",
    "    class_prop_AUCs = []\n",
    "    for i in range(num_parties):\n",
    "        delta = np.array(deltas[i])\n",
    "        Es.append(np.sum(delta[:-1] * np.arange(num_candidate_points-1, 0, -1)))\n",
    "        props = np.array(class_props[i])\n",
    "        class_prop_AUCs.append(np.sum(props[:-1] * np.arange(num_candidate_points-1, 0, -1)))\n",
    "    \n",
    "    all_Es.append(Es)\n",
    "    all_class_prop_AUCs.append(class_prop_AUCs)\n",
    "    \n",
    "    all_corrcoeff.append(np.corrcoef(np.array(list(zip(Es, class_prop_AUCs))).T)[0, 1])\n",
    "\n",
    "print(\"Average correlation coefficient: {}\".format(np.mean(all_corrcoeff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cluster in range(num_clusters):\n",
    "    reward = 9\n",
    "    R = ccr_all_res[cluster]\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    plt.scatter(test_sets[cluster, :, 0], test_sets[cluster, :, 1], s=20, color=cm.get_cmap('Set1')(0*(1/9)), label=\"Party\")\n",
    "    for i in range(num_clusters):\n",
    "        plt.scatter(test_sets[i, :, 0], test_sets[i, :, 1], s=0.1, color='grey')\n",
    "    \n",
    "    added = np.array(R[reward])\n",
    "    alphas = [1-i*(1/len(R[reward])) for i in range(len(R[reward]))]\n",
    "    rgba_colors = np.zeros((len(R[reward]),4))\n",
    "    rgba_colors[:, 3] = alphas\n",
    "    rgba_colors[:, :3] = (0.21568627450980393, 0.49411764705882355, 0.7215686274509804)\n",
    "    plt.scatter(added[:, 0], added[:, 1], s=20, color=rgba_colors, label=\"Added\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Cluster {}, $\\phi = {}$\".format(cluster, phi[reward]))\n",
    "    \n",
    "    plt.xlabel(\"$x_0$\")\n",
    "    plt.ylabel(\"$x_1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD: varying precision hyperparameter $\\eta$ (permutation sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.mmd import perm_sampling\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_candidate_points = 10000\n",
    "num_parties = 10\n",
    "\n",
    "gmm_clusters = [sample_GMM(means, covs, num_candidate_points) for i in range(num_clusters)]\n",
    "X = gmm_clusters[0][0]\n",
    "Y = gmm_clusters[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=d))\n",
    "kernel.base_kernel.lengthscale = [1, 1]\n",
    "kernel.outputscale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_etas = np.linspace(np.log(0.025), np.log(1.), 10)\n",
    "etas = np.exp(log_etas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "etas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_samps = []\n",
    "for eta in etas:\n",
    "    samps = perm_sampling(X[:4000], Y[:4000], kernel, eta=eta)\n",
    "    all_samps.append(samps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "\n",
    "plt.plot(etas, [np.std(samp) for samp in all_samps], label=\"Permutation sampling\")\n",
    "plt.plot(etas, 0.0001*np.sqrt(1/etas), label=\"$O(\\sqrt {1/\\eta})$\")\n",
    "    \n",
    "plt.xlabel(\"$\\eta$\")\n",
    "plt.ylabel(\"Standard deviation\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_x = []\n",
    "all_density = []\n",
    "for i in range(len(all_samps)):\n",
    "    bins = np.histogram(all_samps[i], bins=50)[1]\n",
    "    interval = bins[1] - bins[0]\n",
    "    bins = np.concatenate(([bins[0] - interval*i for i in range(7, 0, -1)], bins))\n",
    "    density = stats.gaussian_kde(all_samps[i])\n",
    "    n, x, _ = plt.hist(all_samps[0], bins=bins, \n",
    "                   histtype=u'step', density=True)  \n",
    "    all_x.append(x)\n",
    "    all_density.append(density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "plt.title(\"Effect of $\\eta$ on variance of $\\widehat{MMD}^2$ distribution\")\n",
    "\n",
    "for i in range(len(all_samps)):\n",
    "    x = all_x[i]\n",
    "    density = all_density[i]\n",
    "    plt.plot(x, density(x), label=\"$\\eta = {}$\".format(etas[i]), color=cm.get_cmap('Spectral')(i*0.1), linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.xlabel(\"$\\widehat{MMD}^2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect on number of points distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_candidate_points = 10000\n",
    "num_parties = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gmm_clusters = [sample_GMM(means, covs, num_candidate_points) for i in range(num_clusters)]\n",
    "gmm = np.array([pair[0] for pair in gmm_clusters])\n",
    "clusters = np.array([pair[1] for pair in gmm_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reference = sample_GMM(means, covs, num_samples)[0]\n",
    "candidates = np.array([gmm[0]]*num_parties)\n",
    "D = np.array([test_sets[0]] * num_parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phi = np.zeros(num_parties)\n",
    "greeds = np.ones(num_parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=d))\n",
    "kernel.base_kernel.lengthscale = [1, 1]\n",
    "kernel.outputscale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_etas = np.linspace(np.log(0.025), np.log(1.), 10)\n",
    "etas = np.exp(log_etas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "etas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eta_all_res = []\n",
    "eta_all_deltas = []\n",
    "eta_all_mus = []\n",
    "\n",
    "for eta in etas:\n",
    "    res, deltas, mus = con_div(candidates, reference, phi, D, kernel, num_perms=1000, greeds=greeds, eta=eta)\n",
    "    eta_all_res.append(res)\n",
    "    eta_all_deltas.append(deltas)\n",
    "    eta_all_mus.append(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[len(res[0]) for res in eta_all_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((gmm, clusters, reference, candidates, phi, test_sets, etas, eta_all_res, eta_all_deltas, eta_all_mus), open(\"CD-alletas.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD: varying greed hyperparameter $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_candidate_points = 10000\n",
    "num_parties = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phi = [0] * num_parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gmm_clusters = [sample_GMM(means, covs, num_candidate_points) for i in range(num_clusters)]\n",
    "gmm = np.array([pair[0] for pair in gmm_clusters])\n",
    "clusters = np.array([pair[1] for pair in gmm_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reference = sample_GMM(means, covs, num_samples)[0]\n",
    "candidates = np.array([gmm[0]]*num_parties)\n",
    "D = np.array([test_sets[2]] * num_parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "greeds = list(np.exp(np.linspace(-2, 2, num_parties-2)))\n",
    "greeds.insert(0, 0)\n",
    "greeds.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=d))\n",
    "kernel.base_kernel.lengthscale = [1, 1]\n",
    "kernel.outputscale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eta = 100/((len(candidates) + len(reference))/2) \n",
    "res, deltas, mus = con_div(candidates, reference, phi, D, kernel, num_perms=1000, greeds=greeds, eta=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "plt.plot(greeds[:-1], [len(result) for result in res[:-1]])\n",
    "plt.hlines(len(res[-1]), xmin=0, xmax=greeds[-2], color=cm.get_cmap('Set1')(0*(1/9)), label=\"Greedy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Greed factor\")\n",
    "plt.ylabel(\"Number of points added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_props = []\n",
    "for result in res:\n",
    "    class_props.append(class_proportion(get_classes(np.array(result), gmm[0], clusters[0]) + [2 for i in range(len(D[0]))], num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "\n",
    "for i in range(num_parties):\n",
    "    plt.plot(greeds[:-1], [prop[1] for prop in class_props[:-1]])\n",
    "    \n",
    "plt.xlabel(\"$\\gamma$\")\n",
    "plt.ylabel(\"Class imbalance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for party in range(num_parties):\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    #plt.gca().set_aspect('equal', adjustable='box')\n",
    "    for i in range(num_clusters):\n",
    "        if i != party:\n",
    "            plt.scatter(test_sets[i, :, 0], test_sets[i, :, 1], s=0.1, color='grey')\n",
    "    plt.scatter(D[party, :, 0], D[party, :, 1], s=10, color=cm.get_cmap('Set1')(0*(1/9)), label=\"Party {}\".format(party))\n",
    "\n",
    "    added = np.array(res[party])\n",
    "    alphas = [1-i*(1/len(added)) for i in range(len(added))]\n",
    "    rgba_colors = np.zeros((len(added),4))\n",
    "    rgba_colors[:, 3] = alphas\n",
    "    rgba_colors[:, :3] = (0.21568627450980393, 0.49411764705882355, 0.7215686274509804)\n",
    "    plt.scatter(added[:, 0], added[:, 1], s=10, color=rgba_colors, label=\"Added\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
